# StableDiffusionEO

StableDiffusionEOé¡¹ç›®æºè‡ª[shenlan2017/TensorRT-StableDiffusion: The Project of the Model Deployment course on ShenLan College](https://github.com/shenlan2017/TensorRT-StableDiffusion)é¡¹ç›®çš„export_onnxåˆ†æ”¯




## é¡¹ç›®é…ç½®
```python
pip install -r requirements.txt
```
æˆ–è€…ä½¿ç”¨docker
```python
docker pull shenlan2017/tensorrt-stable-diffusion:latest
```


## æœ¬é¡¹ç›®å®ç°çš„å†…å®¹

- [x] æ¡ˆä¾‹ï¼šCNSDæ¨¡å‹çš„torch-onnx-trtæ¨¡å‹è½¬æ¢ä»¥åŠtrtæ¨¡å‹è°ƒç”¨

  - [x] å°†CNSDæ¨¡å‹çš„å››ä¸ªæ¨¡å‹ä»torchæ¨¡å‹è½¬æ¢ä¸ºonnxæ¨¡å‹ï¼›ï¼ˆ**å«æ£€æµ‹torchå’Œonnxæ¨¡å‹è¾“å‡ºå·®å¼‚çš„æ–¹æ³•**ï¼‰ï¼›

    export_onnx_all.py

  - [x] å°†CNSDæ¨¡å‹çš„å››ä¸ªæ¨¡å‹ä»onnxæ¨¡å‹è½¬æ¢æˆtrtæ¨¡å‹ï¼›

    onnx2trt_static.py

  - [x] CNSDæ¨¡å‹åŸºäºpytorchæºç è¿›è¡Œè¿è¡Œã€‚

    å½“cldmæ–‡ä»¶å¤¹å¤åˆ¶ä¸ºcldm_torchæ–‡ä»¶å¤¹ï¼Œldmæ–‡ä»¶å¤¹å¤åˆ¶ä¸ºldm_torchæ–‡ä»¶å¤¹æ—¶ï¼Œé¡¹ç›®ä»compute_score.pyä½œä¸ºå…¥å£ï¼Œå°†åŸºäºpytorchæ¨¡å‹è¿›è¡Œè¿è¡Œï¼›

  - [x] CNSDæ¨¡å‹åŸºäºtrtæ¨¡å‹è¿›è¡Œè¿è¡Œã€‚

    å½“cldmæ–‡ä»¶å¤¹å¤åˆ¶ä¸ºcldm_trtæ–‡ä»¶å¤¹ï¼Œldmæ–‡ä»¶å¤¹å¤åˆ¶ä¸ºldm_trtæ–‡ä»¶å¤¹æ—¶ï¼Œé¡¹ç›®ä»compute_score.pyä½œä¸ºå…¥å£ï¼Œå°†åŸºäºtrtæ¨¡å‹è¿›è¡Œè¿è¡Œï¼›

- [x] æ¡ˆä¾‹ï¼šYOLOV5æ¨¡å‹çš„torch-onnx-trtæ¨¡å‹è½¬æ¢ä»¥åŠtrtæ¨¡å‹è°ƒç”¨

  - [x] ~~å°†æ¨¡å‹çš„å››ä¸ªæ¨¡å‹ä»torchæ¨¡å‹è½¬æ¢ä¸ºonnxæ¨¡å‹~~ï¼›

    practice_yolov5\export_onnx_yolov5.py 

    ï¼ˆé‰´äºæœ¬é¡¹ç›®ä¸åŒ…å«yolov5æºç ï¼Œæš‚èˆå»è¯¥å®ç°ï¼ŒğŸ“•**ä¸è¿‡éœ€è¦æ³¨æ„onnxç”Ÿæˆæ—¶çš„inputnameå’Œoutputname**ï¼‰

  - [x] å°†YOLOV5æ¨¡å‹çš„å››ä¸ªæ¨¡å‹ä»onnxæ¨¡å‹è½¬æ¢æˆtrtæ¨¡å‹

    practice_yolov5\onnx2trt_static_yolov5.py

  - [x] æ£€æµ‹onnxå’Œtrtæ¨¡å‹è¾“å‡ºå·®å¼‚çš„æ–¹æ³•ã€‚

    practice_yolov5\trt_yolov5.py

    - **å«trtæ¨¡å‹å’Œonnxæ¨¡å‹åˆå§‹åŒ–æ–¹æ³•ï¼›**
    - **å«trtæ¨¡å‹å’Œonnxæ¨¡å‹è°ƒç”¨æ–¹æ³•ï¼›**
    - å«æ•°æ®é¢„å¤„ç†æ“ä½œï¼›ï¼ˆPreProcessorï¼‰
    - å«æ•°æ®åå¤„ç†æ“ä½œï¼›ï¼ˆPostProcessorï¼‰
    - å«æ£€æµ‹onnxå’Œtrtæ¨¡å‹è¾“å‡ºå·®å¼‚çš„æ–¹æ³•ï¼›ï¼ˆcheck_onnx_trt_outputsï¼‰



## ä½œä¸šéœ€è¦æ³¨æ„çš„å†…å®¹
### ç¬¬äº”ç« --ç¬¬ä¸€æ¬¡ä½œä¸š


### ç¬¬ä¸ƒç« --ç¬¬äºŒæ¬¡ä½œä¸š


### ç¬¬å…«ç« --ç¬¬ä¸‰æ¬¡ä½œä¸š
å†…å®¹ï¼šåˆå¹¶CrossAttentionçš„qkvå±‚
æ³¨æ„ï¼šqkvå±‚ä¿®æ”¹ä¹‹åï¼Œåœ¨è½¬æ¢æˆonnxæ—¶ï¼Œonnx_checkå‡½æ•°ä¼šæŠ¥ä¸torchæ¨¡å‹è¾“å‡ºä¸ä¸€è‡´çš„é”™è¯¯ï¼Œæ˜¯å› ä¸ºself.qkv_wå’Œself.kv_wå¹¶æ²¡æœ‰å‡ºç°åœ¨CNSDçš„æŒä¹…åŒ–å‚æ•°æ–‡ä»¶ä¸­ï¼Œå› æ­¤
å„ä¸ªattentionå±‚ä¸­çš„qã€kã€våœ¨æ¨¡å‹åˆå§‹åŒ–åï¼Œä¼šç»è¿‡æŒä¹…åŒ–æ¨¡å‹çš„ä¿®æ”¹ï¼›
è€Œå…¶ä¸­çš„self.qkv_wå’Œself.kv_wæ˜¯åœ¨æ¨¡å‹åˆå§‹åŒ–åï¼Œç”±äºä¹‹å‰çš„æŒä¹…åŒ–æ¨¡å‹ä¸­å¹¶ä¸åŒ…å«è¯¥å‚æ•°ï¼Œå› æ­¤ä¸ä¼šå¯¹è¯¥å‚æ•°è¿›è¡Œé‡å†™ã€‚    

## çŸ¥è¯†ç‚¹æ±‡æ€»

### CNSDæ¨¡å‹ç»“æ„

![image-20250117134510563](README.assets/image-20250117134510563.png)



### ONNXæ¨¡å‹åˆå§‹åŒ–å’Œè°ƒç”¨æ–¹æ³•

```python
# onnxæ¨¡å‹åˆå§‹åŒ–
input_dicts = {"images":image_tensor.numpy()}
sess = rt.InferenceSession(onnx_path) 
# onnxæ¨¡å‹è°ƒç”¨
outputs_onnx = sess.run(None, input_dicts)[0]             
```



### TRTæ¨¡å‹è½¬æ¢

```python
def onnx2trt(onnxFile, plan_name, min_shapes, opt_shapes, max_shapes, max_workspace_size = None, use_fp16=False, builder_opt_evel=None):
    logger = trt.Logger(trt.Logger.VERBOSE)                                                         # create logger
    builder = trt.Builder(logger)                                                                   # create builder
    config = builder.create_builder_config()                                                        # create config
    network = builder.create_network(1<<int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))      # create network
    
    if max_workspace_size:                                                                          # init config
        config.max_workspace_size = max_workspace_size
    else:
        config.max_workspace_size = 10<<30 # 10GB

    parser = trt.OnnxParser(network, logger)                                                         # create parser
    if not os.path.exists(onnxFile):
        print("Failed finding onnx file!")
        exit()
    print("Succeeded finding onnx file!")
    with open(onnxFile, 'rb') as model:
        # import pdb; pdb.set_trace()
        (onnx_path, _) = os.path.split(onnxFile)
        if not parser.parse(model.read(), path=onnxFile):                                            # parse onnx
            print("Failed parsing ONNX file!")
            for error in range(parser.num_errors):
                print(parser.get_error(error))
            exit()
    print("Succeeded parsing ONNX file!")

    if use_fp16:                                                                                      # init config
        config.set_flag(trt.BuilderFlag.FP16)
        plan_name = plan_name.replace(".plan", "_fp16.plan")

    if builder_opt_evel:
        config.builder_optimization_level = builder_opt_evel

    # set profile
    assert network.num_inputs == len(min_shapes)
    assert network.num_inputs == len(opt_shapes)
    assert network.num_inputs == len(max_shapes)

    profile = builder.create_optimization_profile()                                                   # create profile
    for i in range(network.num_inputs):                                                               # set profile
        input_tensor = network.get_input(i)
        profile.set_shape(input_tensor.name, tuple(min_shapes[i]), tuple(opt_shapes[i]), tuple(max_shapes[i]))

    config.add_optimization_profile(profile)                                                          # init config

    engine = builder.build_engine(network, config)                                                    # create engine
    if not engine:
        raise RuntimeError("build_engine failed")
    print("Succeeded building engine!")

    print("Serializing Engine...")
    serialized_engine = engine.serialize()                                                             # serialize engine
    if serialized_engine is None:
        raise RuntimeError("serialize failed")

    (plan_path, _) = os.path.split(plan_name)
    os.makedirs(plan_path, exist_ok=True)
    with open(plan_name, "wb") as fout:
        fout.write(serialized_engine)
```



### TRTæ¨¡å‹åˆå§‹åŒ–å’Œè°ƒç”¨æ–¹æ³•

```python
# trtæ¨¡å‹åˆå§‹åŒ–
clip_engine = engine_from_bytes(bytes_from_path(engine_path))
model_feed_dict = {
            'images': (1, 3, 1120, 1120),
            'output0': (1, 77175, 14)
        }
context = clip_engine.create_execution_context()
clip_engine.allocate_buffers(model_feed_dict)
clip_engine.get_engine_infor()
# trtæ¨¡å‹è°ƒç”¨
outputs_trt = clip_engine.infer({"images":image_tensor})[onnx_names["output"]].cpu().detach().numpy()
"""
noerror = self.context.execute_async_v3(0)
"""
```

![image-20250117134920795](README.assets/image-20250117134920795.png)



```python

class Engine():
    def __init__(
        self,
        engine_path,
    ):
        self.engine_path = engine_path
        self.engine = None
        self.context = None
        self.buffers = OrderedDict()
        self.tensors = OrderedDict()
        self.latent_h = 1120
        self.latent_w = 1120
        self.batch_size = 1
        self.cuda_graph_instance = None # cuda graph
        
    def yolov5_model_shape_dict(self):
        return {
            'images': (self.batch_size, 3, self.latent_h, self.latent_w),
            'output0': (self.batch_size, 77175, 14)
        }
    def __del__(self):
        [buf.free() for buf in self.buffers.values() if isinstance(buf, cuda.DeviceArray) ]
        del self.engine
        del self.context
        del self.buffers
        del self.tensors

    def load(self):
        print(f"Loading TensorRT engine: {self.engine_path}")
        self.engine = engine_from_bytes(bytes_from_path(self.engine_path))

    def activate(self, reuse_device_memory=None):
        """æ¿€æ´»TensorRTå¼•æ“çš„æ‰§è¡Œä¸Šä¸‹æ–‡ã€‚
        """
        # å¦‚æœæä¾›äº† reuse_device_memoryï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ²¡æœ‰åˆ†é…æ–°è®¾å¤‡å†…å­˜çš„æ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºå½“å‰ä¸Šä¸‹æ–‡
        if reuse_device_memory:
            self.context = self.engine.create_execution_context_without_device_memory()
            self.context.device_memory = reuse_device_memory
        else:  # å¦‚æœæ²¡æœ‰æä¾› reuse_device_memoryï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„æ‰§è¡Œä¸Šä¸‹æ–‡
            self.context = self.engine.create_execution_context()

    def allocate_buffers(self, shape_dict=None, device='cuda'):
        """ä¸º TensorRT å¼•æ“åˆ†é…è¾“å…¥å’Œè¾“å‡ºç¼“å†²åŒºã€‚
        """
        # éå†æ¯ä¸ªç»‘å®šï¼ˆè¾“å…¥æˆ–è¾“å‡ºï¼‰
        for idx in range(trt_util.get_bindings_per_profile(self.engine)):
            binding = self.engine[idx]                 # è·å–ç»‘å®šçš„åç§°
            if shape_dict and binding in shape_dict:   # å¦‚æœæä¾›äº† shape_dict å¹¶ä¸”ç»‘å®šåœ¨ shape_dict ä¸­ï¼Œåˆ™ä½¿ç”¨ shape_dict ä¸­çš„å½¢çŠ¶
                shape = shape_dict[binding]
            else:
                shape = self.engine.get_binding_shape(binding)         # å¦åˆ™ï¼Œä½¿ç”¨å¼•æ“çš„é»˜è®¤å½¢çŠ¶
            dtype = trt.nptype(self.engine.get_binding_dtype(binding)) # è·å–ç»‘å®šçš„æ•°æ®ç±»å‹
            if self.engine.binding_is_input(binding):        # å¦‚æœç»‘å®šæ˜¯è¾“å…¥ï¼Œåˆ™è®¾ç½®ä¸Šä¸‹æ–‡çš„ç»‘å®šå½¢çŠ¶
                self.context.set_binding_shape(idx, shape)
            tensor = torch.empty(tuple(shape), dtype=numpy_to_torch_dtype_dict[dtype]).to(device=device)
            self.tensors[binding] = tensor                   # å°†å¼ é‡æ·»åŠ åˆ° tensors å­—å…¸ä¸­ï¼Œé”®ä¸ºç»‘å®šçš„åç§°
            
    def get_engine_infor(self):
        # è®¡ç®—è¾“å…¥å¼ é‡çš„æ•°é‡
        nInput = np.sum([self.engine.binding_is_input(i) for i in range(self.engine.num_bindings)])
        # è®¡ç®—è¾“å‡ºå¼ é‡çš„æ•°é‡
        nOutput = self.engine.num_bindings - nInput
        # è·å–è¾“å…¥å¼ é‡çš„åç§°å’Œå½¢çŠ¶
        input_infor = dict((self.engine.get_tensor_name(i), self.context.get_binding_shape(i))  for i in range(nInput))
        # è·å–è¾“å‡ºå¼ é‡çš„åç§°å’Œå½¢çŠ¶
        ouput_infor = dict((self.engine.get_tensor_name(nInput + i), self.context.get_binding_shape(nInput + i))  for i in range(nOutput))
        print("TensorRT engine infors -----------------")
        print("engin nInput: ", nInput, ", Input shape: ", input_infor)
        print("engin nOutput: ", nOutput, ", Outpu shape: ", ouput_infor)

    def infer(self, feed_dict, stream=None, use_cuda_graph=False):
        # import pdb; pdb.set_trace()
        # éå† feed_dict ä¸­çš„æ¯ä¸ªé”®å€¼å¯¹ï¼Œå°†å€¼å¤åˆ¶åˆ°å¯¹åº”çš„å¼ é‡ä¸­
        for name, buf in feed_dict.items():
            self.tensors[name].copy_(buf)

        # éå† tensors ä¸­çš„æ¯ä¸ªé”®å€¼å¯¹ï¼Œè®¾ç½®å¼ é‡çš„åœ°å€
        for name, tensor in self.tensors.items():
            self.context.set_tensor_address(name, tensor.data_ptr())

        # å¦‚æœä½¿ç”¨ CUDA å›¾ä¼˜åŒ–
        if use_cuda_graph:
            
            if self.cuda_graph_instance is not None:  # å¦‚æœå·²ç»æœ‰ä¸€ä¸ª CUDA å›¾å®ä¾‹
                CUASSERT(cudart.cudaGraphLaunch(self.cuda_graph_instance, stream.ptr))  # å¯åŠ¨ CUDA å›¾å®ä¾‹
                CUASSERT(cudart.cudaStreamSynchronize(stream.ptr))                      # åŒæ­¥ CUDA æµ
            else:                                     # å¦‚æœæ²¡æœ‰ CUDA å›¾å®ä¾‹
                # do inference before CUDA graph capture
                noerror = self.context.execute_async_v3(stream.ptr)
                if not noerror: raise ValueError(f"ERROR: inference failed.")   # å¦‚æœæ¨ç†å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                # capture cuda graph
                CUASSERT(cudart.cudaStreamBeginCapture(stream.ptr, cudart.cudaStreamCaptureMode.cudaStreamCaptureModeGlobal))                # å¼€å§‹æ•è· CUDA æµ
                self.context.execute_async_v3(stream.ptr)    # æ‰§è¡Œæ¨ç†
                self.graph = CUASSERT(cudart.cudaStreamEndCapture(stream.ptr))   # ç»“æŸæ•è· CUDA æµ
                self.cuda_graph_instance = CUASSERT(cudart.cudaGraphInstantiate(self.graph, 0))  # å®ä¾‹åŒ– CUDA å›¾
        else:
            if stream:
                noerror = self.context.execute_async_v3(stream.ptr)
            else:
                noerror = self.context.execute_async_v3(0)
            if not noerror:
                raise ValueError(f"ERROR: inference failed.")

        return self.tensors
```



### æ£€æµ‹æ¨¡å‹è½¬æ¢å‰åè¾“å‡ºå·®å¼‚

```python
## ä»¥yoloä¸ºä¾‹
# 1 æ¨¡å‹è¾“å‡º [bs, anchors_number, 5 + nc]
ret = np.allclose(model_trt_outputs, model_onnx_outputs, rtol=1e-03, atol=1e-05, equal_nan=False)
# 2 æœ€ç»ˆä»»åŠ¡çš„è¾“å‡ºç»“æœ [bs, achors_number, 6]
# 3 polygraphè¾“å‡ºonnxå’Œtrtæ¨¡å‹çš„æ¯å±‚è¾“å‡ºï¼ˆæ’æŸ¥nanæº¢å‡ºæ—¶ï¼‰
# 4 
```





### FP16ä¼˜åŒ–çš„ç†è®ºä¸å®ç°æ–¹å¼





### buildoptimalçš„ç†è®ºä¸å®ç°æ–¹å¼



### cudaå›¾ä¼˜åŒ–çš„ç†è®ºä¸å®ç°æ–¹å¼



### pipelineä¼˜åŒ–çš„æ–¹é¢ï¼šè¿­ä»£æ¬¡æ•°ã€æ‹¼batchã€æ¨¡å‹å†…éƒ¨ä¼˜åŒ–



### int8é‡åŒ–



### æ·±åº¦ä¼˜åŒ–






## TODO

- [ ] cudaå’Œtrtçš„è¯­æ³•ä½“ç³»æ¯”è¾ƒæ¬ ç¼ºï¼›

- [ ] YOLOV5æ¨¡å‹ç»“æ„

- [ ] FP16ä¼˜åŒ–çš„ç†è®ºä¸å®ç°æ–¹å¼ï¼›

  

- [ ] cudaå›¾ä¼˜åŒ–çš„ç†è®ºä¸å®ç°æ–¹å¼ï¼›

- [ ] buildoptimalçš„ç†è®ºä¸å®ç°æ–¹å¼ï¼›

- [ ] pipelineä¼˜åŒ–çš„æ–¹é¢ï¼šè¿­ä»£æ¬¡æ•°ã€æ‹¼batchã€æ¨¡å‹å†…éƒ¨ä¼˜åŒ–ï¼›

- [ ] int8é‡åŒ–ï¼›

- [ ] int4é‡åŒ–ï¼›

- [ ] æ·±åº¦ä¼˜åŒ–

- [ ] å‰ªæï¼›

- [ ] è’¸é¦ï¼›

- [ ] æ‹¼batch
